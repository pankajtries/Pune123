# -*- coding: utf-8 -*-
"""Prac7(textana).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xayEpEfluePPsu-t9uulaX7dkfNjvKEP
"""

import nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
from nltk.tokenize import word_tokenize, sent_tokenize, TreebankWordTokenizer, WordPunctTokenizer, TweetTokenizer, MWETokenizer
from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer
from nltk import pos_tag

text = "NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources."

"""### **Tokenization**"""

characters = list(text)
print("Character Tokenization:")
print(characters)
print("Length of characters list:", len(characters))

tokens = word_tokenize(text)
print("\nWord Tokenization:")
print(tokens)
print("Length of words list:", len(words))

sentences = sent_tokenize(text)
print("\nSentence Tokenization:")
print(sentences)
print("Length of sentences list:", len(sentences))

treebank_tokenizer = TreebankWordTokenizer()
words_treebank = treebank_tokenizer.tokenize(text)
print("\nTreebankWordTokenizer:")
print(words_treebank)
print("Length of words list (TreebankWordTokenizer):", len(words_treebank))

wordpunct_tokenizer = WordPunctTokenizer()
words_wordpunct = wordpunct_tokenizer.tokenize(text)
print("\nWordPunctTokenizer:")
print(words_wordpunct)
print("Length of words list (WordPunctTokenizer):", len(words_wordpunct))

tweet_tokenizer = TweetTokenizer()
words_tweet = tweet_tokenizer.tokenize(text)
print("\nTweetTokenizer:")
print(words_tweet)
print("Length of words list (TweetTokenizer):", len(words_tweet))

mwe_tokenizer = MWETokenizer()
words_mwe = mwe_tokenizer.tokenize(text.split())
print("\nMWETokenizer:")
print(words_mwe)
print("Length of words list (MWETokenizer):", len(words_mwe))

from textblob import TextBlob

blob = TextBlob(text)
words_blob = blob.words
print("TextBlob Word Tokenization:")
print(words_blob)
print("Length of words list (TextBlob):", len(words_blob))



"""### **Stemming**"""

porter_stemmer = PorterStemmer()

print("Original Word\t\tPorter Stemmed Word")
for word in words:
    stemmed_word = porter_stemmer.stem(word)
    print(f"{word:20}\t\t{stemmed_word}")

snowball_stemmer = SnowballStemmer(language='english')
stemmed_snowball = [snowball_stemmer.stem(word) for word in words]
print("\nStemming using SnowballStemmer:")
print(stemmed_snowball)

lancaster_stemmer = LancasterStemmer()
stemmed_lancaster = [lancaster_stemmer.stem(word) for word in words]
print("\nStemming using LancasterStemmer:")
print(stemmed_lancaster)

"""### **Lemmatization**"""

from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

lemmatizer = WordNetLemmatizer()

lemmatized_words = [lemmatizer.lemmatize(token) for token in tokens]

print("WordNet Lemmatization:")
print(lemmatized_words)

import spacy
nlp = spacy.load("en_core_web_sm")

doc = nlp(text)

lemmatized_words = [token.lemma_ for token in doc]

print("spaCy Lemmatization:")
print(lemmatized_words)

"""### **POS Tagging**"""

print(pos_tag(tokens))