{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnmcEv3UxbwW",
        "outputId": "52b49529-f692-4bee-c248-015ae06f3fc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, TreebankWordTokenizer, WordPunctTokenizer, TweetTokenizer, MWETokenizer\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
        "from nltk import pos_tag\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "shnz4FWmtng_"
      },
      "outputs": [],
      "source": [
        "text = \"NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG50vGF0ERwJ"
      },
      "source": [
        "### **Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7MOIu5DxpIE",
        "outputId": "be6e00e7-424f-41da-87a0-1803d6b8ad17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Character Tokenization:\n",
            "['N', 'L', 'T', 'K', ' ', 'i', 's', ' ', 'a', ' ', 'l', 'e', 'a', 'd', 'i', 'n', 'g', ' ', 'p', 'l', 'a', 't', 'f', 'o', 'r', 'm', ' ', 'f', 'o', 'r', ' ', 'b', 'u', 'i', 'l', 'd', 'i', 'n', 'g', ' ', 'P', 'y', 't', 'h', 'o', 'n', ' ', 'p', 'r', 'o', 'g', 'r', 'a', 'm', 's', ' ', 't', 'o', ' ', 'w', 'o', 'r', 'k', ' ', 'w', 'i', 't', 'h', ' ', 'h', 'u', 'm', 'a', 'n', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'd', 'a', 't', 'a', '.', ' ', 'I', 't', ' ', 'p', 'r', 'o', 'v', 'i', 'd', 'e', 's', ' ', 'e', 'a', 's', 'y', '-', 't', 'o', '-', 'u', 's', 'e', ' ', 'i', 'n', 't', 'e', 'r', 'f', 'a', 'c', 'e', 's', ' ', 't', 'o', ' ', 'o', 'v', 'e', 'r', ' ', '5', '0', ' ', 'c', 'o', 'r', 'p', 'o', 'r', 'a', ' ', 'a', 'n', 'd', ' ', 'l', 'e', 'x', 'i', 'c', 'a', 'l', ' ', 'r', 'e', 's', 'o', 'u', 'r', 'c', 'e', 's', '.']\n",
            "Length of characters list: 166\n"
          ]
        }
      ],
      "source": [
        "characters = list(text)\n",
        "print(\"Character Tokenization:\")\n",
        "print(characters)\n",
        "print(\"Length of characters list:\", len(characters))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLrw25pjxpul",
        "outputId": "dc501f30-143f-42a1-dacb-5dd60eff7938"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Tokenization:\n",
            "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.', 'It', 'provides', 'easy-to-use', 'interfaces', 'to', 'over', '50', 'corpora', 'and', 'lexical', 'resources', '.']\n",
            "Length of words list: 28\n"
          ]
        }
      ],
      "source": [
        "tokens = word_tokenize(text)\n",
        "print(\"\\nWord Tokenization:\")\n",
        "print(tokens)\n",
        "print(\"Length of words list:\", len(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvrsU-Mdxroc",
        "outputId": "0c4115b1-933e-422b-8869-8a8409ac2feb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence Tokenization:\n",
            "['NLTK is a leading platform for building Python programs to work with human language data.', 'It provides easy-to-use interfaces to over 50 corpora and lexical resources.']\n",
            "Length of sentences list: 2\n"
          ]
        }
      ],
      "source": [
        "sentences = sent_tokenize(text)\n",
        "print(\"\\nSentence Tokenization:\")\n",
        "print(sentences)\n",
        "print(\"Length of sentences list:\", len(sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKk3cLl_ySh1",
        "outputId": "d9d3a456-99b7-485a-baf8-5827c13adba6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TreebankWordTokenizer:\n",
            "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data.', 'It', 'provides', 'easy-to-use', 'interfaces', 'to', 'over', '50', 'corpora', 'and', 'lexical', 'resources', '.']\n",
            "Length of words list (TreebankWordTokenizer): 27\n"
          ]
        }
      ],
      "source": [
        "treebank_tokenizer = TreebankWordTokenizer()\n",
        "words_treebank = treebank_tokenizer.tokenize(text)\n",
        "print(\"\\nTreebankWordTokenizer:\")\n",
        "print(words_treebank)\n",
        "print(\"Length of words list (TreebankWordTokenizer):\", len(words_treebank))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwSvYcQuyVLs",
        "outputId": "5ab04c87-7fc7-45df-dadf-c2d62a8796b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WordPunctTokenizer:\n",
            "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.', 'It', 'provides', 'easy', '-', 'to', '-', 'use', 'interfaces', 'to', 'over', '50', 'corpora', 'and', 'lexical', 'resources', '.']\n",
            "Length of words list (WordPunctTokenizer): 32\n"
          ]
        }
      ],
      "source": [
        "wordpunct_tokenizer = WordPunctTokenizer()\n",
        "words_wordpunct = wordpunct_tokenizer.tokenize(text)\n",
        "print(\"\\nWordPunctTokenizer:\")\n",
        "print(words_wordpunct)\n",
        "print(\"Length of words list (WordPunctTokenizer):\", len(words_wordpunct))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBfbtPjyyhpr",
        "outputId": "ee20cf83-c3cd-4140-8c69-2b505814e29c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TweetTokenizer:\n",
            "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.', 'It', 'provides', 'easy-to-use', 'interfaces', 'to', 'over', '50', 'corpora', 'and', 'lexical', 'resources', '.']\n",
            "Length of words list (TweetTokenizer): 28\n"
          ]
        }
      ],
      "source": [
        "tweet_tokenizer = TweetTokenizer()\n",
        "words_tweet = tweet_tokenizer.tokenize(text)\n",
        "print(\"\\nTweetTokenizer:\")\n",
        "print(words_tweet)\n",
        "print(\"Length of words list (TweetTokenizer):\", len(words_tweet))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6_2_bdfyjqz",
        "outputId": "1e3007b1-3eeb-4825-8164-bc3ef9d91c83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MWETokenizer:\n",
            "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data.', 'It', 'provides', 'easy-to-use', 'interfaces', 'to', 'over', '50', 'corpora', 'and', 'lexical', 'resources.']\n",
            "Length of words list (MWETokenizer): 26\n"
          ]
        }
      ],
      "source": [
        "mwe_tokenizer = MWETokenizer()\n",
        "words_mwe = mwe_tokenizer.tokenize(text.split())\n",
        "print(\"\\nMWETokenizer:\")\n",
        "print(words_mwe)\n",
        "print(\"Length of words list (MWETokenizer):\", len(words_mwe))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYyHXS-SymRD",
        "outputId": "ebb2a1aa-437e-4c9b-c3cd-09d17a938420"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TextBlob Word Tokenization:\n",
            "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', 'It', 'provides', 'easy-to-use', 'interfaces', 'to', 'over', '50', 'corpora', 'and', 'lexical', 'resources']\n",
            "Length of words list (TextBlob): 26\n"
          ]
        }
      ],
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "blob = TextBlob(text)\n",
        "words_blob = blob.words\n",
        "print(\"TextBlob Word Tokenization:\")\n",
        "print(words_blob)\n",
        "print(\"Length of words list (TextBlob):\", len(words_blob))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "7oh5yBsN0GQp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nM0XVj-OEISG"
      },
      "source": [
        "### **Stemming**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c5QfHfE0SD5",
        "outputId": "d4e941fc-a217-4306-d5d5-9436c0a5662b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Word\t\tPorter Stemmed Word\n",
            "NLTK                \t\tnltk\n",
            "is                  \t\tis\n",
            "a                   \t\ta\n",
            "leading             \t\tlead\n",
            "platform            \t\tplatform\n",
            "for                 \t\tfor\n",
            "building            \t\tbuild\n",
            "Python              \t\tpython\n",
            "programs            \t\tprogram\n",
            "to                  \t\tto\n",
            "work                \t\twork\n",
            "with                \t\twith\n",
            "human               \t\thuman\n",
            "language            \t\tlanguag\n",
            "data                \t\tdata\n",
            ".                   \t\t.\n",
            "It                  \t\tit\n",
            "provides            \t\tprovid\n",
            "easy-to-use         \t\teasy-to-us\n",
            "interfaces          \t\tinterfac\n",
            "to                  \t\tto\n",
            "over                \t\tover\n",
            "50                  \t\t50\n",
            "corpora             \t\tcorpora\n",
            "and                 \t\tand\n",
            "lexical             \t\tlexic\n",
            "resources           \t\tresourc\n",
            ".                   \t\t.\n"
          ]
        }
      ],
      "source": [
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "print(\"Original Word\\t\\tPorter Stemmed Word\")\n",
        "for word in words:\n",
        "    stemmed_word = porter_stemmer.stem(word)\n",
        "    print(f\"{word:20}\\t\\t{stemmed_word}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1r3MJvYY0vZS",
        "outputId": "7621de47-a87d-42ec-e2f8-8ee1007e13d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stemming using SnowballStemmer:\n",
            "['nltk', 'is', 'a', 'lead', 'platform', 'for', 'build', 'python', 'program', 'to', 'work', 'with', 'human', 'languag', 'data', '.', 'it', 'provid', 'easy-to-us', 'interfac', 'to', 'over', '50', 'corpora', 'and', 'lexic', 'resourc', '.']\n"
          ]
        }
      ],
      "source": [
        "snowball_stemmer = SnowballStemmer(language='english')\n",
        "stemmed_snowball = [snowball_stemmer.stem(word) for word in words]\n",
        "print(\"\\nStemming using SnowballStemmer:\")\n",
        "print(stemmed_snowball)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNKebVvI0xm5",
        "outputId": "b3052d6d-3da7-4b1d-c68a-90547ec57235"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stemming using LancasterStemmer:\n",
            "['nltk', 'is', 'a', 'lead', 'platform', 'for', 'build', 'python', 'program', 'to', 'work', 'with', 'hum', 'langu', 'dat', '.', 'it', 'provid', 'easy-to-use', 'interfac', 'to', 'ov', '50', 'corpor', 'and', 'lex', 'resourc', '.']\n"
          ]
        }
      ],
      "source": [
        "lancaster_stemmer = LancasterStemmer()\n",
        "stemmed_lancaster = [lancaster_stemmer.stem(word) for word in words]\n",
        "print(\"\\nStemming using LancasterStemmer:\")\n",
        "print(stemmed_lancaster)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTB3Swx8EMAc"
      },
      "source": [
        "### **Lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FooLO2DD07DB",
        "outputId": "3f7c000c-5766-4f0d-b2ed-875e8b83394f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WordNet Lemmatization:\n",
            "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'program', 'to', 'work', 'with', 'human', 'language', 'data', '.', 'It', 'provides', 'easy-to-use', 'interface', 'to', 'over', '50', 'corpus', 'and', 'lexical', 'resource', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatized_words = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "print(\"WordNet Lemmatization:\")\n",
        "print(lemmatized_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHyZhQjfFz53",
        "outputId": "1fbd0a35-659c-46a0-c4bc-c6884b9bdb13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy Lemmatization:\n",
            "['NLTK', 'be', 'a', 'lead', 'platform', 'for', 'build', 'Python', 'program', 'to', 'work', 'with', 'human', 'language', 'datum', '.', 'it', 'provide', 'easy', '-', 'to', '-', 'use', 'interface', 'to', 'over', '50', 'corpora', 'and', 'lexical', 'resource', '.']\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "lemmatized_words = [token.lemma_ for token in doc]\n",
        "\n",
        "print(\"spaCy Lemmatization:\")\n",
        "print(lemmatized_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4iKYB-NGCRS"
      },
      "source": [
        "### **POS Tagging**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4edP2t_GFdi",
        "outputId": "fcd66735-4edd-4af4-b336-619cd42d674b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('leading', 'VBG'), ('platform', 'NN'), ('for', 'IN'), ('building', 'VBG'), ('Python', 'NNP'), ('programs', 'NNS'), ('to', 'TO'), ('work', 'VB'), ('with', 'IN'), ('human', 'JJ'), ('language', 'NN'), ('data', 'NNS'), ('.', '.'), ('It', 'PRP'), ('provides', 'VBZ'), ('easy-to-use', 'JJ'), ('interfaces', 'NNS'), ('to', 'TO'), ('over', 'IN'), ('50', 'CD'), ('corpora', 'NNS'), ('and', 'CC'), ('lexical', 'JJ'), ('resources', 'NNS'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "print(pos_tag(tokens))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "nM0XVj-OEISG"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}